package lexer

// Lexer tokenizes markdown input using a state machine pattern.
// Each state function examines the input and returns the next state.
type Lexer struct {
    input   string  // The complete markdown input string
    start   int     // Start position of current token
    pos     int     // Current position in input
    width   int     // Width of last rune read
    line    int     // Current line number (1-indexed)
    column  int     // Current column number (1-indexed)
    tokens  []Token // Collected tokens
    context Context // Current parsing context
}

// stateFunc represents a state in the lexer state machine.
// Each state function processes input according the rules of its state.
// Returns either the next state function to execute or nil (indicating lexing is complete).
type stateFunc func(*Lexer) stateFunc

// New creates a new lexer for the given input string.
func New(input string) *Lexer {
    return &Lexer{
        input: input,
        position: 0,
    }
}

func (l *Lexer) Tokenize() []Token {
    var tokens []Token

    for l.position < len(l.input) {
        tokens = append(tokens, *l.nextToken())
    }

    return tokens
}

func (l *Lexer) nextToken() *Token {
    readChar := l.peek()

    switch {
    case readChar == 0:
        l.advance()
        return &Token{Type: EOF, Value: ""}

    default:
        text :=  ""
        for !isTokenChar(l.peek()) {
            text += string(l.advance())
        }
        return &Token{Type: TEXT, Value: text}
    }
}

func (l *Lexer) peek() rune {
    if l.position >= len(l.input) {
        return 0
    }
    return rune(l.input[l.position])
}

func (l *Lexer) advance() rune {
    char := l.peek()
    l.position++
    return char
}
